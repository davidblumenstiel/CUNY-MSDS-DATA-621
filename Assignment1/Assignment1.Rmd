---
title: "Assignment1"
author: "David Blumenstiel"
date: "2/23/2021"
output: html_document
---
```{r}
library(knitr)
library(tidyverse)
library(reshape2)
library(VIM)
library(corrplot)
library(naniar)
library(caret)
library(GGally)
library(MASS)
```


```{r}
testing <- read.csv("C:\\Users\\blume\\OneDrive\\Desktop\\CUNY MSDS\\Data 621 Buisness Analytics and Data Mining\\Assignment_1\\moneyball-evaluation-data.csv")

training <-read.csv("C:\\Users\\blume\\OneDrive\\Desktop\\CUNY MSDS\\Data 621 Buisness Analytics and Data Mining\\Assignment_1\\moneyball-training-data.csv")

```

```{r}
summary(training)
summary(testing)
```


```{r message=FALSE, warning=FALSE,fig.width=10, fig.height= 9}
#There is somthing wrong with this; the correlations are off
training %>% 
  cor(., use = "complete.obs") %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", tl.cex=.8, diag = FALSE)
```



```{r}
cleanup <- function(df, outlier_mult = 1.5) {  
  #Outlier_mult is what mutliple of the IQR a value needs to be away from the median to be considered an outlier
  
  #Change NA to median where appropriate
  df <- df %>% replace_na(list(TEAM_BATTING_SO = median(df$TEAM_BATTING_SO[(is.na(df$TEAM_BATTING_SO) == FALSE)]),
                               TEAM_BASERUN_SB = median(df$TEAM_BASERUN_SB[(is.na(df$TEAM_BASERUN_SB) == FALSE)]),
                               TEAM_BASERUN_CS = median(df$TEAM_BASERUN_CS[(is.na(df$TEAM_BASERUN_CS) == FALSE)]),
                               TEAM_PITCHING_SO = median(df$TEAM_PITCHING_SO[(is.na(df$TEAM_PITCHING_SO) == FALSE)]),
                               TEAM_FIELDING_DP = median(df$TEAM_FIELDING_DP[(is.na(df$TEAM_FIELDING_DP) == FALSE)])
                               ))
  
  #Drop column with too many NA
  df$TEAM_BATTING_HBP  <- NULL
  df$INDEX <- NULL
  
  #Removes with outliers
  i = 1 #Skip the index
  k = 0 #Count of outliers changed
  while (i < length(colnames(df))) {   #Column cycle
    
    i = i+1
    iqr= IQR(df[,i])
    med = median(df[,i])
    max_range = c(med - iqr * outlier_mult, med + iqr * outlier_mult)  #Defines the maximum range, outside of which values are considered outliers
      
    j = 0
    while (j < length(df[,i])) { #Row cycle
      j = j + 1
      
      if (df[j,i] < max_range[1] || df[j,i] > max_range[2] ) {
        
        df[j,i] <- med #Sets outliers to median column value 
        k = k + 1
      }
    }
  }
  
  
 
  print(paste("set",k, "outliers to median"))
  return(df)
}
```

Now we'll use the function to clean the data.  In addition, we'll also do a box-cox transformation on the responce variable.

```{r}
#Clean all the data
testing <- cleanup(testing, outlier_mult = 3)  
training <- cleanup(training, outlier_mult = 3)

#Guide for this bit found here https://www.r-bloggers.com/2016/01/how-to-detect-heteroscedasticity-and-rectify-it/
boxcox_trans_mod <- BoxCoxTrans(training$TARGET_WINS)
training$TARGET_WINS_transformed <- predict(boxcox_trans_mod, training$TARGET_WINS)

summary(training)
```


```{r}

#par(mfrow=c(4,4))
names = colnames(training)
for (i in 3:length(training)-1) {
  name = names[i]
  plot(training$TARGET_WINS_transformed ~ training[,i], xlab = name, ylab = "TARGET_WINS_transformed")
  l = lm(training$TARGET_WINS_transformed ~ training[,i])
  abline(l, col = "red")
  text(min(training[,i]),130,paste("R^2 = ", round(summary(l)$r.squared,3),"  cor = ",round(cor(y = training$TARGET_WINS_transformed, x =  training[,i]),3)), adj = 0)
}


```

### Model 1

This model performs the best of the two I've made.  It utilizes all variables except the pitching ones, which are pretty much co-linear with the batting ones.  This uses a stepwise method to narrow down the parameters, and checks all interactions terms 2 layers deep.  5 fold cv is used with 2 repeats.

```{r}
set.seed(1234567890) #So you see the same thing I do

#Set's up some K-fold validation and repetition
tr <- trainControl(method = "repeatedcv", number = 5, repeats = 2)

#Set's up the model
model <- train(TARGET_WINS_transformed~ (TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_FIELDING_E + TEAM_FIELDING_DP)^2, data = training, method = "lmStepAIC", trControl= tr)

#Plots it out and gets stats  
plot(model$finalModel)
predictions <- predict(model, training)
plot(predictions, training$TARGET_WINS)
summary(model$finalModel)


```
This one performs fairly well, with R^2 near 0.4.  Doing the box-cox transformaton improved the R^2 by about 0.03.  The tails on the QQ plot are a bit heavy, and scale, but residuals and leveage look good.  My biggest concern is slight heterocedasticity.  It also uses predictors which, by themselves, are not very predictive.


### Model 2
This model is similar to the last one, but narrows down the number of parameters considered to only those with a decent correlation to target wins. This one goes three layers deep into interaction terms, and repeats twice more.

```{r}
tr <- trainControl(method = "repeatedcv", number = 5, repeats = 2, verboseIter = TRUE)

model2 <- train(TARGET_WINS_transformed~ (TEAM_BATTING_H + TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR + TEAM_BATTING_BB)^3, data = training, method = "lmStepAIC", trControl= tr)

plot(model2$finalModel)
predictions <- predict(model2, training)
plot(predictions, training$TARGET_WINS)
summary(model2$finalModel)

```



```{r}
predictions <- predict(model, training)
plot(predictions, training$TARGET_WINS)
```












